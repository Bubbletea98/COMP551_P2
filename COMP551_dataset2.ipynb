{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk import word_tokenize        \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train=load_files('aclImdb/train/', description=None, \n",
    "                                        categories=['neg', 'pos'], load_content=True, shuffle=True,\n",
    "                                        encoding=None, decode_error='strict', random_state=0)\n",
    "movie_test=load_files('aclImdb/test/', description=None, \n",
    "                                        categories=['neg', 'pos'], load_content=True, shuffle=True,\n",
    "                                        encoding=None, decode_error='strict', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True,min_df=3,tokenizer=LemmaTokenizer())),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',LogisticRegression()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__C':[1.0,1.5,2.0,2.5,3.0,4.0,5.0],'clf__tol': [1e-4],'clf__max_iter':[400],'clf__solver':['lbfgs']}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24dd4526e2fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m40000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00000000001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstoplist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtfidf_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Train data retrieve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(C= 2.0, max_iter= 40000, solver= 'lbfgs', tol=0.00000000001)\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True,tokenizer=LemmaTokenizer())\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "#Train data retrieve\n",
    "X_train_counts = count_vect.fit_transform(movie_train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "lg.fit(X_train_tfidf, movie_train.target)\n",
    "#Test data retrieve\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "#Print the test accuracy\n",
    "t1_start = time.time() \n",
    "predicted = lg.predict(X_test_tfidf)\n",
    "t1_end = time.time() \n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print('mean',np.mean(predicted==movie_test.target))\n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True)),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',tree.DecisionTreeClassifier()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__max_depth':[50,100,200,400,None],'clf__max_features':[None],'clf__splitter':['random','best']}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied sub and subtest as the traindata and testdata, improved accuracy by 0.005\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_counts = count_vect.fit_transform(movie_train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#Y_train_tfidf = tfidf_transformer.fit_transform(Y_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "DT_clf = tree.DecisionTreeClassifier(max_depth=None, max_features=None, max_leaf_nodes=None, splitter='random')\n",
    "DT_clf=DT_clf.fit(X_train_tfidf,movie_train.target )\n",
    "predicted = DT_clf.predict(X_test_tfidf)\n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print(np.mean(predicted ==movie_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True)),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',AdaBoostClassifier()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__learning_rate':[0.1,0.2,0.5,1.0],'clf__n_estimators':[50,100]}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_counts = count_vect.fit_transform(movie_train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#Y_train_tfidf = tfidf_transformer.fit_transform(Y_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "t1_start = time.time() \n",
    "DT_clf = AdaBoostClassifier(learning_rate=0.5,n_estimators=1000)\n",
    "DT_clf=DT_clf.fit(X_train_tfidf,movie_train.target )\n",
    "predicted = DT_clf.predict(X_test_tfidf)\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)\n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print(np.mean(predicted ==movie_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True)),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',LinearSVC()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__loss':['hinge','squared_hinge'],'clf__tol':[1e-4,1e-6,1e-7,1e-9,1e-11],'clf__max_iter':[100000]}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_counts = count_vect.fit_transform(movie_train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#Y_train_tfidf = tfidf_transformer.fit_transform(Y_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "t1_start = time.time() \n",
    "SVC_clf = LinearSVC(loss='squared_hinge',max_iter=100000,tol= 0.0001)\n",
    "SVC_clf=SVC_clf.fit(X_train_tfidf,movie_train.target )\n",
    "predicted = SVC_clf.predict(X_test_tfidf)\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)\n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print(np.mean(predicted ==movie_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6df12be0fdfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mt1_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmovie_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rank_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True)),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',RandomForestClassifier()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__n_estimators':[50,100,200,300],'clf__max_depth':[1,None]}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_counts = count_vect.fit_transform(movie_train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#Y_train_tfidf = tfidf_transformer.fit_transform(Y_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "t1_start = time.time() \n",
    "RF_clf = RandomForestClassifier(max_depth=None,n_estimators= 300,n_jobs=-1)\n",
    "RF_clf=RF_clf.fit(X_train_tfidf,movie_train.target )\n",
    "predicted = RF_clf.predict(X_test_tfidf)\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)\n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print(np.mean(predicted ==movie_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stoplist.append('\\n')\n",
    "stoplist.append(' ')\n",
    "stoplist.append('\"')\n",
    "stoplist.append('|')\n",
    "stoplist.append('/')\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stoplist,lowercase=True)),\n",
    "    ('norm',Normalizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',MultinomialNB()),\n",
    " ])\n",
    "\n",
    "tuned_parameters= [{'clf__alpha':[0.01,0.015,0.0195,0.02,0.025,0.03,0.04,0.05]}]\n",
    "t1_start = time.time() \n",
    "clf=GridSearchCV(text_clf, tuned_parameters, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "clf.fit(movie_train.data,movie_train.target)\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_['rank_test_score'])\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "count_vect=CountVectorizer(stop_words=stoplist,lowercase=True)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#Y_train_tfidf = tfidf_transformer.fit_transform(Y_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(movie_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "t1_start = time.time() \n",
    "RF_clf = MultinomialNB(alpha=0.0190)\n",
    "RF_clf=RF_clf.fit(X_train_tfidf,movie_train.target )\n",
    "predicted = RF_clf.predict(X_test_tfidf)\n",
    "t1_end = time.time() \n",
    "print('processing time is :',t1_end-t1_start)\n",
    "print(classification_report(movie_test.target, predicted))\n",
    "print(np.mean(predicted ==movie_test.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
